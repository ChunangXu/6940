import threading
import requests
import json
import argparse
import time
from tqdm import tqdm
from constants import PROMPTS
from collections import defaultdict, Counter
from queue import Queue


# Parse CLI arguments
parser = argparse.ArgumentParser(description="Collecting data for APIs")
parser.add_argument("--url", type=str, required=True, help="OpenAI-compatible API endpoint")
parser.add_argument("--api_key", type=str, required=True, help="OpenAI API key")
parser.add_argument("--model", type=str, required=True, help="Model to use for completion")
parser.add_argument("--temperature", type=float, default=0.0, help="Sampling temperature for the model")
parser.add_argument('--sample_size', type=int, default=None, help='Sample size for the prompt list')
args = parser.parse_args()

# Clean and prepare prompt list
SAMPLE_SIZE = args.sample_size
unique_prompt_set = list(set(PROMPTS.split("\n")))
if isinstance(SAMPLE_SIZE, int) and SAMPLE_SIZE > 0:
    prompt_set = [p.strip() for p in unique_prompt_set[:SAMPLE_SIZE] if p.strip()]
else:
    prompt_set = [p.strip() for p in unique_prompt_set if p.strip()]

# Create a queue with each prompt 20 times
prompt_queue = Queue()
for prompt in prompt_set:
    for _ in range(20):
        prompt_queue.put(prompt)

# Initialize thread-safe data structures
lock = threading.Lock()
test_dict = defaultdict(Counter)
failed_count = defaultdict(int)  # Track per-prompt failures
MAX_RETRIES = 10  # Max retries for each failed prompt


def extract_actual_first_token(generated_text):
    """
    Extract the first token of the actual answer after skipping the entire block
    of text that is the reasoning/chain-of-thought part.

    :param generated_text: Full text generated by the LLM.
    :param tag_pairs: List of tuples for known start/end tags (e.g., [("<think>", "</think>")]).
    :return: First token (str) or None if no answer found.
    """
    # Tag pairs for reasoning sections
    tag_pairs = [
        ("<think>", "</think>"),
        ("<reasoning>", "</reasoning>"),
        ("<cot>", "</cot>"),
        ("<|im_start|>assistant\n<|im_start|>", "<|im_end|>"),
        ("<|im_start|>", "<|im_end|>")
    ]

    stripped_text = generated_text.strip()
    actual_answer = None

    # Try to find and remove the block of reasoning/chain-of-thought text
    for start_tag, end_tag in tag_pairs:
        if stripped_text.startswith(start_tag):
            start_idx = len(start_tag)
            end_idx = stripped_text.find(end_tag, start_idx)

            if end_idx != -1:
                # If end tag was found, then the actual generated answer starts
                # after the end tag
                answer_start = end_idx + len(end_tag)
                actual_answer = stripped_text[answer_start:].strip()
            else:
                # If no end tag was found, then `generated_text` does not contain
                # the actual answer
                actual_answer = None
            break

    # Start tag was not found so we use full generated text
    if actual_answer is None:
        actual_answer = stripped_text

    # Return the first token
    return actual_answer.split()[0]


def get_response(progress_bar):
    while not prompt_queue.empty():
        try:
            prompt = prompt_queue.get_nowait()
        except:
            return

        headers = {
            "Authorization": f"Bearer {args.api_key}",
            "Content-Type": "application/json"
        }

        system_msg = (
            "You are a helpful assistant.\n"
            "When the user gives you an unfinished sentence, "
            "respond with ONLY the single next word that best completes it.\n"
            "Do not output chain-of-thought, tags, explanations, or punctuation."
        )

        payload = {
            "model": args.model,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": prompt}
            ],
            "temperature": args.temperature,
            "max_tokens": 1,
        }

        try:
            response = requests.post(f"{args.url}/chat/completions", headers=headers, json=payload)
            response.raise_for_status()
            response_json = response.json()

            content = response_json["choices"][0]["message"]["content"].strip()
            word = extract_actual_first_token(content)

            with lock:
                test_dict[prompt][word] += 1

                # Reset the failure count of a prompt on success
                failed_count[prompt] = 0
                progress_bar.update(1)

        except Exception as e:
            print(f"Error: {e}")
            with lock:
                failed_count[prompt] += 1

                if failed_count[prompt] <= MAX_RETRIES:
                    # Add the prompt back to the `prompt_queue` for retry
                    prompt_queue.put(prompt)
                else:
                    progress_bar.write(f"Permanent failure on prompt: {prompt}")

            # Exponential delay (1s, 3s, 9s, 27s, .., 60s)
            delay = min(3 ** (failed_count[prompt] - 1), 60)
            time.sleep(delay)


def run_threads(num_threads, progress_bar):
    threads = []

    for _ in range(num_threads):
        thread = threading.Thread(target=get_response, args=(progress_bar,))
        thread.start()
        threads.append(thread)

    for thread in threads:
        thread.join()


# Main execution
if __name__ == "__main__":
    total_requests = len(prompt_set) * 20
    print(f"Using {len(prompt_set)} unique prompts, totaling {total_requests} requests")
    print("===== Starting data collection =====")

    # Create tqdm progress bar
    with tqdm(total=total_requests, desc="Collecting data", unit="req", mininterval=1.0, smoothing=0.05,
              bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} "
                         "[{elapsed}<{remaining}, {rate_fmt}{postfix}]") as progress_bar:

        while not prompt_queue.empty():
            batch_size = min(200, prompt_queue.qsize())
            run_threads(batch_size, progress_bar)

        progress_bar.set_postfix_str("Done!", refresh=True)

    print("All requests completed!")

    # Convert defaultdict(Counter) to regular dicts for JSON serialization
    result_dict = {prompt: dict(counter) for prompt, counter in test_dict.items()}

    if "/" in args.model:
        filename = f"./results/{args.model.split('/')[1]}_results_{args.temperature}.json"
    else:
        filename = f"./results/{args.model}_results_{args.temperature}.json"

    with open(filename, "w") as f:
        json.dump(result_dict, f, indent=2)

    print(f"Results saved to {filename}")